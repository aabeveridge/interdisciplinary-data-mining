# *Interdisciplinary Data Mining: Best Practices and Research Methodologies for Analyzing Social Media and Digital Content*

## Authors: Aaron Beveridge and Nicholas M. Van Horn

## Introduction
The introduction extends the abstract (see: `README.md`) and explains how students, faculty, and non-academic researchers can get the most from *Interdisciplinary Data Mining*. Because this book has been written in R markdown, all of the example data visualizations in the methodologies section (Section II) also serve as functioning code examples to support readers in conducting their own studies. Each chapter provides suggested readings and tutorials to assist researchers in reproducing similar data visualizations for their own projects, and all of the code examples are available through GitHub. Additionally, the introduction will address why we have organized the book the way we have, providing a rationale for our choices and providing alternative ways of exploring the content in the book.

**Introduction Outline:**  
- Extend abstract
- What is data mining?
  - Data Science vs. Big Data
- Rationale for outline and approach to book
- How to get the most out of this book
- Why R?
- How to contribute to MassMine

## Section I: Best Practices
### Chapter 1: Being a Good Citizen of the Internet
This chapter explains the underlying rationale for MassMine: the use Application Programming Interfaces (APIs) to collect data from digital networks. Building on a basic description of API scraping, as compared to web scraping and web crawling, Chapter 1 explains the benefits and limitations of API scraping for interdisciplinary research.

**Chapter 1 Outline:**  
- What are APIs and why do they exist?
- API scraping vs web scraping vs web crawling
- Scraping best practices
  - Robots.txt
  - Page visit delays
  - Bandwidth limitations of APIs
- Potential pitfalls/mishaps to avoid
  - Accidental DDOS
  - Unintended Doxing  
- Conclusion
  - How other chapters extend the concept of "Being a Good Citizen of the Internet"

### Chapter 2: Developer (Researcher) Guidelines, Terms of Service, and Network Blogs and Histories
It's a common maxim to hear people proclaim: "I never read the documentation" or "I do not pay attention to terms of service." This chapter explains why researchers who are interested in interdisciplinary data mining cannot proceed with ignorance regarding the developer rules and terms of service for APIs. Additionally, it is important to pay attention to the ways in which various networks change over time, and how they account for these changes through developer blogs and public statements made about changes to their underlying systems and technologies.

**Chapter 2 Outline:**  
- Why read the guidelines and terms of service?
- What can be found here?

### Chapter 3: Ethics: Surveillance, Privacy, and Intellectual Property

## Section II: Research Methodologies

### Chapter 4: Graphs, Game Theory, and Diffusion of Innovations

### Chapter 5: Natural Language Processing
Outline:  
- Text as Data
  - Brief History of NLP
  - Bag of Words vs. Word Embeddings
  - Scrubbing and Data Janitorial Work
  - Word Frequencies and N-Grams
  - Word Correlations
  -
- Documents and Corpora
- Emoji
- Speech-to-Text: Analyzing Audio and Video

### Chapter 6: Demographics, Psychometrics, and Mixed-Methods

## Section III: Collecting, Sharing, and Exporting Data

### Chapter 7: Managing a Data Collection Project

### Chapter 8: Exploratory and Descriptive Statistics

### Chapter 9: Exporting, Sharing, and Managing Large Datasets
